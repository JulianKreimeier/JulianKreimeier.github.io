<section>
    <div class="container">

        <hr>
        <h4 id="2020">2020</h4>
        <hr>

        <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                <img style="padding: 5%" width="100%"
                     src="assets/img/TwoDecades.png">
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12" style="padding:2%">
                <span class="papertitle">Two Decades of Touchable and Walkable Virtual Reality for Blind and Visually Impaired People: A High-Level Taxonomy</span>
                <br>
                J. Kreimeier<span style="font-weight: lighter">and T. Goetzelmann</span>
                <br>
                <em>Multimodal Technologies and interaction </em>2020
                <br>
                <p class="d-inline-flex gap-1">
                    <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseTwoDecades" role="button"
                       aria-expanded="false" aria-controls="collapseTwoDecades">
                        Abstract
                    </a>
                    <a class="btn btn-dark"
                       href="https://www.mdpi.com/2414-4088/4/4/79">MDPI</a>
                </p>
                <div class="collapse" id="collapseTwoDecades">
                    <div class="card card-body">
                        <p>
                            Although most readers associate the term virtual reality (VR) with visually appealing entertainment content, this technology also promises to be helpful to disadvantaged people like blind or visually impaired people. While overcoming physical objects' and spaces' limitations, virtual objects and environments that can be spatially explored have a particular benefit. To give readers a complete, clear and concise overview of current and past publications on touchable and walkable audio supplemented VR applications for blind and visually impaired users, this survey paper presents a high-level taxonomy to cluster the work done up to now from the perspective of technology, interaction and application. In this respect, we introduced a classification into small-, medium- and large-scale virtual environments to cluster and characterize related work. Our comprehensive table shows that especially grounded force feedback devices for haptic feedback ("small scale") were strongly researched in different applications scenarios and mainly from an exocentric perspective, but there are also increasingly physically ("medium scale") or avatar-walkable ("large scale") egocentric audio-haptic virtual environments. In this respect, novel and widespread interfaces such as smartphones or nowadays consumer grade VR components represent a promising potential for further improvements. Our survey paper provides a database on related work to foster the creation process of new ideas and approaches for both technical and methodological aspects.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                <img style="padding: 5%" width="100%"
                     src="assets/img/BikeVR.png">
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12" style="padding:2%">
                <span class="papertitle">BikeVR: A Virtual Reality Bicycle Simulator towards Sustainable Urban Space and Traffic Planning</span>
                <br>
                <span style="font-weight: lighter">D. Ullmann, </span>J. Kreimeier<span style="font-weight: lighter">, T. Goetzelmann, and H. Kipke</span>
                <br>
                <em>Proceedings of Mensch und Computer </em>2020
                <br>
                <p class="d-inline-flex gap-1">
                    <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseBikeVR" role="button"
                       aria-expanded="false" aria-controls="collapseBikeVR">
                        Abstract
                    </a>
                    <a class="btn btn-dark"
                       href="https://dl.acm.org/doi/abs/10.1145/3404983.3410417">ACM</a>
                </p>
                <div class="collapse" id="collapseBikeVR">
                    <div class="card card-body">
                        <p>
                            While becoming more and more aware of the ongoing climate change, eco-friendly means of transport for all citizens are moving further into focus. In order to be able to implement specific measures, it is necessary to better understand and emphasize sustainable transportation like walking and cycling through focused research. When developing novel traffic concepts and urban spaces for non-motorized traffic participants like bicycles and pedestrians, traffic and urban planning must be focused on their needs. To provide rare qualitative factors (such as stress, the perception of time and attractiveness of the environment) in this context, we present an audiovisual VR bicycle simulator which allows the user to cycle through a virtual urban environment by physically pedaling and also steering. Virtual Reality (VR) is a suitable tool in this context, as study participants find identical and almost freely definable (virtual) urban spaces with adjustable traffic scenarios. Our preliminary prototype proved to be promising and will be further optimized and evaluated.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                <img style="padding: 5%" width="100%"
                     src="assets/img/Wheelchair2020.png">
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12" style="padding:2%">
                <span class="papertitle">Towards the Inclusion of Wheelchair Users in Smart City Planning Through Virtual Reality Simulation</span>
                <br>
                <span style="font-weight: lighter">T. Goetzelmann and </span>J. Kreimeier
                <br>
                <em>International Conference on PErvasive Technologies Related to Assistive Environments </em>2020
                <br>
                <p class="d-inline-flex gap-1">
                    <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseWheelchair2020" role="button"
                       aria-expanded="false" aria-controls="collapseWheelchair2020">
                        Abstract
                    </a>
                    <a class="btn btn-dark"
                       href="https://dl.acm.org/doi/abs/10.1145/3389189.3398008">ACM</a>
                </p>
                <div class="collapse" id="collapseWheelchair2020">
                    <div class="card card-body">
                        <p>
                            The planning of Smart Cities is a complex task. In particular, accessibility rules based on legal regulations but also on empirical values must be observed. However, it is difficult to determine in advance the exact needs of people with disabilities for concrete planning. Previous approaches mainly aimed at existing urban environments. Ideally, citizens should be directly involved in the planning process of buildings and urban environments. For existing urban environments, crowdsourcing approaches exist to obtain suggestions for improvement from citizens. We present a novel approach for the direct integration of wheelchair users in the urban environments to be planned (participatory urban development) in virtual reality. We present an easy-to-reproduce simulator that allows wheelchair users to directly explore the planned buildings and urban environments in a virtual, spatial environment. This means that these 3D models can be commented already in the planning phase and provide valuable information about accessibility.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                <img style="padding: 5%" width="100%"
                     src="assets/img/ParticipationElderly.png">
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12" style="padding:2%">
                <span class="papertitle">Participation of Elderly People in Smart City Planning by Means of Virtual Reality</span>
                <br>
                <span style="font-weight: lighter">T. Goetzelmann and </span>J. Kreimeier
                <br>
                <em>International Conference on PErvasive Technologies Related to Assistive Environments </em>2020
                <br>
                <p class="d-inline-flex gap-1">
                    <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseParticipationElderly" role="button"
                       aria-expanded="false" aria-controls="collapseParticipationElderly">
                        Abstract
                    </a>
                    <a class="btn btn-dark"
                       href="https://dl.acm.org/doi/abs/10.1145/3389189.3397649">ACM</a>
                </p>
                <div class="collapse" id="collapseParticipationElderly">
                    <div class="card card-body">
                        <p>
                            Urbanisation is progressing, the population in many countries has a growing proportion of old people. This must be taken into account when transforming urban environments into Smart Cities. On the one hand, general accessibility rules must be taken into account in the planning of buildings and urban environments. However, an essential requirement for transformation is the participation of citizens, i.e., concrete suggestions for improvement from citizen should be taken into account. Elderly people are a valuable source of information for improvements. Instead of simply involving them in the modification of existing facilities, our approach suggests that they should be included in the planning process. However, abstract plans and questionnaires allow only limited insights for ordinary citizen. Therefore, our approach aims at providing a suitable interface to interactively walk through and annotate virtual reality plans for buildings and city districts. We present a working prototype for elderly people based on 3D consumer hardware appropriate form of locomotion with which they can explore and annotate urban planning according to their individual needs.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                <img style="padding: 5%" width="100%"
                     src="assets/img/BlindwalkVR.png">
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12" style="padding:2%">
                <span class="papertitle">BlindwalkVR: Formative Insights Into Blind and Visually Impaired People's VR Locomotion Using Commercially Available Approaches</span>
                <br>
                J. Kreimeier<span style="font-weight: lighter">, P. Karg, and T. Goetzelmann</span>
                <br>
                <em>International Conference on PErvasive Technologies Related to Assistive Environments </em>2020
                <br>
                <p class="d-inline-flex gap-1">
                    <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseBlindWalkVR" role="button"
                       aria-expanded="false" aria-controls="collapseBlindWalkVR">
                        Abstract
                    </a>
                    <a class="btn btn-dark"
                       href="https://dl.acm.org/doi/abs/10.1145/3389189.3389193">ACM</a>
                </p>
                <div class="collapse" id="collapseBlindWalkVR">
                    <div class="card card-body">
                        <p>
                            Virtual Reality (VR) promises expanded access to spatial information, especially for blind and visually impaired people. Through haptic and acoustic feedback, real world's limitations like the risk of injury or the necessity of a sighted safety assistant can be circumvented. However, the best possible profit of this technology requires interactive locomotion in large virtual environments to overcome real world space limitations. Thus, we present formative insights of blind people's egocentric VR locomotion by comparing four different implementations (i.e., two VR treadmills, trackers on the ankles or joystick based locomotion) in a qualitative and quantitative user study with seven blind and visually impaired participants. Our results reveal novel insights on characteristics of each implementation in terms of usability and practicability and also provide recommendations for further work in this field with the target user group in sight.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                <img style="padding: 5%" width="100%"
                     src="assets/img/Optimization.png">
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12" style="padding:2%">
                <span class="papertitle">Optimization of Navigation Considerations of People With Visual Impairments Through Ambient Intelligence: A Case Study</span>
                <br>
                <span style="font-weight: lighter">T. Goetzelmann and </span>J. Kreimeier
                <br>
                <em>International Conference on PErvasive Technologies Related to Assistive Environments </em>2020
                <br>
                <p class="d-inline-flex gap-1">
                    <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseOptimization" role="button"
                       aria-expanded="false" aria-controls="collapseOptimization">
                        Abstract
                    </a>
                    <a class="btn btn-dark"
                       href="https://dl.acm.org/doi/abs/10.1145/3389189.3398009">ACM</a>
                </p>
                <div class="collapse" id="collapseOptimization">
                    <div class="card card-body">
                        <p>
                            As urbanization progresses, cities are becoming increasingly complex. To make this complexity advantageous, Smart Cities integrate intelligent sensors that communicate with each other and are invisible to citizens, so that they can offer additional services. Despite the fact that the inclusion of diverse citizens is an essential requirement of Smart Cities, Ambient Intelligence is often not considered under accessibility aspects.
                            The central question in this context is how disadvantaged groups in particular can benefit from Smart Cities. It is extremely important for people with disabilities to be able to move autonomously in public spaces. For this purpose, however, people who are blind sometimes need to be able to ask other people for directions. Knowing whether people are present can be important information. In our exemplary case study, Ambient Intelligence is used for people with visual impairments, so that they can decide whether they want to go to this or an alternative place based on the information about the density of people in a place.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                <img style="padding: 5%" width="100%"
                     src="assets/img/TableTop.png">
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12" style="padding:2%">
                <span class="papertitle">Tabletop Virtual Haptics: Feasibility Study for the Exploration of 2.5 D Virtual Objects by Blind and Visually Impaired With Consumer Data Gloves</span>
                <br>
                J. Kreimeier<span style="font-weight: lighter">, P. Karg, and T. Goetzelmann</span>
                <br>
                <em>International Conference on PErvasive Technologies Related to Assistive Environments </em>2020
                <br>
                <p class="d-inline-flex gap-1">
                    <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseTabletop" role="button"
                       aria-expanded="false" aria-controls="collapseTabletop">
                        Abstract
                    </a>
                    <a class="btn btn-dark"
                       href="https://dl.acm.org/doi/abs/10.1145/3389189.3389194">ACM</a>
                </p>
                <div class="collapse" id="collapseTabletop">
                    <div class="card card-body">
                        <p>
                            When thinking of Virtual Reality (VR), most people think of stunning audio-visual environments in the context of entertainment. However, VR can also provide haptic information, e.g., to convey spatial information to blind and visually impaired people. In this context of accessibility they might be able to explore independently and self-determined tactile graphics, e.g., the structure of unknown real places prior to visiting them. Thus, we propose and evaluate tabletop virtual objects that can be felt by nowadays commercially available VR components, instead of exploring physical models (e.g., 3D printed maps) with the bare hand. These can be easily placed on an empty table, giving the blind user faster and more independent access to tactile information than with real physical representations. Our comprehensive pilot user study shows that it is possible to recognize floor plans and simple geometric shapes in this context. Also, the insights gained with regard to the suitability for practical application in this context point out the way to eased access to spatial (virtual) information using off-the-shelf components which can significantly support blind and visually impaired users' autonomy.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <br>
        <div class="row">
            <div class="col-lg-4 col-md-4 col-sm-12 text-center">
                <img style="padding: 5%" width="100%"
                     src="assets/img/Blindscanline.png">
            </div>
            <div class="col-lg-8 col-md-8 col-sm-12" style="padding:2%">
                <span class="papertitle">Blindscanline: Preliminary Implementation and Evaluation of a Cross-Platform Line Scanning Sonification Approach Using Frequency and Amplitude Modulation</span>
                <br>
                J. Kreimeier<span style="font-weight: lighter">, M. Kappe, and T. Goetzelmann</span>
                <br>
                <em>International Conference on PErvasive Technologies Related to Assistive Environments </em>2020
                <br>
                <p class="d-inline-flex gap-1">
                    <a class="btn btn-dark" data-bs-toggle="collapse" href="#collapseBlindscanline" role="button"
                       aria-expanded="false" aria-controls="collapseBlindscanline">
                        Abstract
                    </a>
                    <a class="btn btn-dark"
                       href="https://dl.acm.org/doi/abs/10.1145/3389189.3393742">ACM</a>
                </p>
                <div class="collapse" id="collapseBlindscanline">
                    <div class="card card-body">
                        <p>
                            Sonification is a promising way for blind and visually impaired people to capture and process information purely auditory, e.g., by mapping distance metrics on sound characteristics. To optimally use the users' sensory bandwidth and cognitive load a sequential scanning at multiple azimuth angles instead of only one measuring point straight ahead could be a suitable option. Thus, we present a preliminary cross-platform implementation and evaluation of such a sequential 'line-scanning sonification' using off-the-shelf components while comparing frequency (FM) and amplitude modulation (AM). In our user study with blindfolded and visually impaired participants, users gained a more accurate mental model in significantly shorter time by means of FM compared to AM and the HoloLens' usability was rated better than our LIDAR prototype's. These initial findings show possibilities for further improvement, so that similar approaches could be used more and better in blind and visually impaired peoples' everyday life.
                        </p>
                    </div>
                </div>
            </div>
        </div>
        <br>

    </div>
</section>